{"cells":[{"cell_type":"markdown","metadata":{"id":"v--J7LUqL9Cj"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25899,"status":"ok","timestamp":1685669741762,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"BnKnD-UQWSEg","outputId":"9875e5ce-572a-44ea-9c58-e14a913ad9c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3803,"status":"ok","timestamp":1685669745560,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"8Af_47xxWVhC","outputId":"fcc67a91-7aeb-45a3-ed74-6a5f606b8d10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n","Requirement already satisfied: typing_extensions\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"]}],"source":["!pip install polars"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31587,"status":"ok","timestamp":1685669777142,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"nWURCxBnSGcH","outputId":"66f35d47-f3a0-4bba-c993-c8d27d6f9997"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gensim==4.0.1\n","  Downloading gensim-4.0.1.tar.gz (23.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy\u003e=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.22.4)\n","Requirement already satisfied: scipy\u003e=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.10.1)\n","Requirement already satisfied: smart_open\u003e=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (6.3.0)\n","Building wheels for collected packages: gensim\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─\u003e\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n","Failed to build gensim\n","\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install gensim==4.0.1"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3726,"status":"ok","timestamp":1685669780862,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"F-RBDJdwWdI-"},"outputs":[],"source":["import os\n","import gc\n","import math\n","import random\n","from collections import defaultdict, Counter\n","from typing import List, Dict\n","import joblib\n","import pickle\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from numba import njit\n","import polars as pl\n","import pandas as pd\n","import lightgbm as lgb\n","from sklearn.model_selection import GroupKFold\n","from gensim.models import Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"hEPw4yzayxut"},"source":["## constants"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1685669780863,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"c3OCFFMJZoNy"},"outputs":[],"source":["EXP_NAME = \"exp147\"\n","DIR = \"/gdrive/MyDrive/amazon_kdd_2023/\"\n","K_FOLDS = 3\n","SEED = 42\n","LOCALES = [\"IT\", \"FR\", \"ES\"]\n","MAKE_TRAIN = True\n","MAKE_VALID = False\n","MAKE_TEST = False\n","\n","# This parameter controls to which end item the candidate is tied. \n","# For example, if [1,2], candidates are generated from the last item and second last item in each session.\n","LAST_NS = [1, 2, 3] "]},{"cell_type":"markdown","metadata":{"id":"f19D1NtjyBUk"},"source":["## load data"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685669780864,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"Gy094WXkDJVb"},"outputs":[],"source":["class CandidateMatrix:\n","    def __init__(self, matrix: pl.DataFrame, feat_name: List[str], join_key: str):\n","        self.matrix = matrix\n","        self.feat_name = feat_name\n","        self.join_key = join_key"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2056,"status":"ok","timestamp":1685669782912,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"LOiuD_bFC4jA"},"outputs":[],"source":["# session data\n","train = pl.read_parquet(DIR + \"data/preprocessed/task2/train_task2.parquet\")\n","test = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase2.parquet\")"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4392,"status":"ok","timestamp":1685669787300,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"drzF-AZlDLso"},"outputs":[],"source":["# feature data\n","session_feat = pl.read_parquet(DIR + \"data/interim/features/task2/session_feature_07.parquet\")\n","product_feat_train = pl.read_parquet(DIR + \"data/interim/features/task2/product_feature_train_09.parquet\")\n","product_feat_test = pl.read_parquet(DIR + \"data/interim/features/task2/product_feature_test_09.parquet\")"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6275,"status":"ok","timestamp":1685669793566,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"illkwaxrDQDV"},"outputs":[],"source":["similar_products = pl.read_parquet(DIR + \"data/interim/candidates/task2/similar_products_17.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8zYqjY06DSYr"},"outputs":[],"source":["if MAKE_TRAIN or MAKE_VALID:\n","    imf_candidates_train_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_31_for_train_or_eval.parquet\")\n","    imf_candidates_train_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_32_for_train_or_eval.parquet\")\n","    imf_candidates_train_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_33_for_train_or_eval.parquet\")\n","    imf_candidates_train_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_34_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_32_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_33_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_34_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_35_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_5 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_36_for_train_or_eval.parquet\")\n","    prone_matrix_train = pl.read_parquet(DIR + \"data/interim/candidates/task2/prone_07_for_local_or_eval.parquet\")\n","    i2v_matrix_train = pl.read_parquet(DIR + \"data/interim/candidates/task2/nns_matrix_19_for_train_or_eval.parquet\")\n","\n","    candidate_matrices_train =[\n","        CandidateMatrix(co_visit_matrix_train_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_2, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_3, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_4, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_5, [\"lift\", \"lift_rank\"], \"item\"),\n","        CandidateMatrix(similar_products, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n","        CandidateMatrix(imf_candidates_train_1, [\"imf_score\", \"imf_rank\"], \"session\"),\n","        CandidateMatrix(imf_candidates_train_2, [\"imf_score_to_recent_intereaction_n3\", \"imf_rank_to_recent_intereaction_n3\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_train_3, [\"imf_score_to_recent_intereaction_n5\", \"imf_rank_to_recent_intereaction_n5\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_train_4, [\"imf_score_to_recent_intereaction_n2\", \"imf_rank_to_recent_intereaction_n2\"], \"session\"),    \n","        CandidateMatrix(prone_matrix_train, [\"prone_distance\", \"prone_rank\"], \"item\"),    \n","        CandidateMatrix(i2v_matrix_train, [\"nns_similality\", \"nns_rank\"], \"item\"),    \n","    ]\n","\n","    # item2vec model\n","    i2v_models_train = {}\n","    for locale in LOCALES:\n","        i2v_models_train[locale] = Word2Vec.load(DIR + f\"models/task2/item2vec_{locale}_16_for_train_or_eval.model\")\n","\n","    # imf model\n","    imf_model_train = np.load(DIR + \"models/task2/imf_31_model_for_train_or_eval.npz\")\n","    with open(DIR + \"models/task2/imf_31_user_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","        user_id2index_train = pickle.load(f)\n","    with open(DIR + \"models/task2/imf_31_item_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","        item_id2index_train = pickle.load(f)\n","    \n","    # prone\n","    graph_embs_train = {}\n","    item_id2indices_prone_train = {}\n","    for locale in LOCALES:\n","        graph_embs_train[locale] = np.load(DIR + f\"models/task2/graph_embedding_07_{locale}_for_local_train_or_eval.npy\")\n","        with open(DIR + \"data/interim/graph/task2/graph_\" + f\"item_id2index_07_{locale}_for_train_or_eval.pickle\", \"rb\") as f:\n","            item_id2indices_prone_train[locale] = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5Sak9XgADSbP"},"outputs":[],"source":["if MAKE_TEST:\n","    imf_candidates_test_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_31_for_inference.parquet\")\n","    imf_candidates_test_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_32_for_inference.parquet\")\n","    imf_candidates_test_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_33_for_inference.parquet\")\n","    imf_candidates_test_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_34_for_inference.parquet\")\n","    co_visit_matrix_test_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_32_for_inference.parquet\")\n","    co_visit_matrix_test_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_33_for_inference.parquet\")\n","    co_visit_matrix_test_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_34_for_inference.parquet\")\n","    co_visit_matrix_test_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_35_for_inference.parquet\")\n","    co_visit_matrix_test_5 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_36_for_inference.parquet\")\n","    prone_matrix_test = pl.read_parquet(DIR + \"data/interim/candidates/task2/prone_07_for_inference.parquet\")\n","    i2v_matrix_test = pl.read_parquet(DIR + \"data/interim/candidates/task2/nns_matrix_19_for_inference.parquet\")\n","\n","    candidate_matrices_test =[\n","        CandidateMatrix(co_visit_matrix_test_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_2, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_3, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_4, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_5, [\"lift\", \"lift_rank\"], \"item\"),\n","        CandidateMatrix(similar_products, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n","        CandidateMatrix(imf_candidates_test_1, [\"imf_score\", \"imf_rank\"], \"session\"),\n","        CandidateMatrix(imf_candidates_test_2, [\"imf_score_to_recent_intereaction_n3\", \"imf_rank_to_recent_intereaction_n3\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_test_3, [\"imf_score_to_recent_intereaction_n5\", \"imf_rank_to_recent_intereaction_n5\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_test_4, [\"imf_score_to_recent_intereaction_n2\", \"imf_rank_to_recent_intereaction_n2\"], \"session\"),    \n","        CandidateMatrix(prone_matrix_test, [\"prone_distance\", \"prone_rank\"], \"item\"),    \n","        CandidateMatrix(i2v_matrix_test, [\"nns_similality\", \"nns_rank\"], \"item\"),    \n","    ]\n","    \n","    # item2vec \n","    i2v_models_test = {}\n","    for locale in LOCALES:\n","        i2v_models_test[locale] = Word2Vec.load(DIR + f\"models/task2/item2vec_{locale}_16_for_inference.model\")\n","\n","    # imf model\n","    imf_model_test = np.load(DIR + \"models/task2/imf_31_model_for_inference.npz\")\n","    with open(DIR + \"models/task2/imf_31_user_id2index_for_inference.pickle\", \"rb\") as f:\n","        user_id2index_test = pickle.load(f)\n","    with open(DIR + \"models/task2/imf_31_item_id2index_for_inference.pickle\", \"rb\") as f:\n","        item_id2index_test = pickle.load(f)\n","\n","    # prone\n","    graph_embs_test = {}\n","    item_id2indices_prone_test = {}\n","    for locale in LOCALES:\n","        graph_embs_test[locale] = np.load(DIR + f\"models/task2/graph_embedding_07_{locale}_for_inference.npy\")\n","        with open(DIR + \"data/interim/graph/task2/graph_\" + f\"item_id2index_07_{locale}_for_inference.pickle\", \"rb\") as f:\n","            item_id2indices_prone_test[locale] = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"E-NbXCWfyDZt"},"source":["## functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"D75acjHUyEyt"},"outputs":[],"source":["# functions for data processing\n","\n","def generate_candidates(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -\u003e pl.DataFrame:\n","\n","    def add_last_n_item(df: pl.DataFrame, last_n: int) -\u003e pl.DataFrame:\n","        last_item_list = []\n","        prev_items_list = df[\"prev_items\"].to_list()\n","        for prev_items in prev_items_list:\n","            try:\n","                last_item_list.append(prev_items[-last_n])\n","            except IndexError:\n","                last_item_list.append(None)\n","        df = df.with_columns(pl.Series(name=f\"last_item_{last_n}\", values=last_item_list))\n","        return df\n","\n","    # add last_item columns\n","    for last_n in LAST_NS:\n","        df = add_last_n_item(df, last_n)\n","\n","    # generate candidates\n","    candidates = []\n","\n","    # candidates tied to items\n","    for last_n in LAST_NS:\n","        for candidate_matrix in candidate_matrices:\n","            if candidate_matrix.join_key == \"item\":\n","                # join candidates to last_n item\n","                candidate = df.join(candidate_matrix.matrix, left_on=[f\"last_item_{last_n}\", \"locale\"], right_on=[\"item\", \"locale\"], how=\"left\")\n","                candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n","\n","                # keep candidates for feature addition later\n","                original_feat_names = candidate_matrix.feat_name\n","                feat_names = [f\"{x}_last{last_n}\" for x in original_feat_names]\n","                tmp = candidate[[\"session_id\", \"candidate_item\"] + original_feat_names]\n","                for original_feat_name, feat_name in zip(original_feat_names, feat_names):\n","                    tmp = tmp.rename({original_feat_name:feat_name})\n","                candidates.append(tmp)\n","\n","    # candidates tied to session\n","    for candidate_matrix in candidate_matrices:\n","        if candidate_matrix.join_key == \"session\":\n","            # join candidates to session\n","            candidate = df.join(candidate_matrix.matrix, on=\"session_id\", how=\"left\")\n","            candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n","\n","            # keep candidates for feature addition later\n","            candidates.append(candidate[[\"session_id\", \"candidate_item\"] + candidate_matrix.feat_name])\n","\n","    cand_all = pl.concat([df[[\"session_id\", \"candidate_item\"]] for df in candidates])\n","\n","    # remove duplicate candidates\n","    cand_all = cand_all.unique(subset=[\"session_id\", \"candidate_item\"])\n","\n","    # join candidates\n","    df = df.join(cand_all, on=[\"session_id\"], how=\"left\")\n","\n","    # add features derived from the candidate\n","    for candidate in candidates:\n","        df = df.join(candidate, on=[\"session_id\", \"candidate_item\"], how=\"left\")\n","\n","    return df\n","\n","def add_label(df: pl.DataFrame) -\u003e pl.DataFrame:\n","    df = df.with_columns((pl.col(\"candidate_item\") == pl.col(\"next_item\")).cast(pl.Int8).alias(\"label\"))\n","    return df\n","\n","def filter_null(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -\u003e pl.DataFrame:\n","    feat_names = []\n","    for candidate_matrix in candidate_matrices:\n","        if candidate_matrix.join_key == \"item\":\n","            for last_n in LAST_NS:\n","                for feat_name in candidate_matrix.feat_name:\n","                    feat_names.append(f\"{feat_name}_last{last_n}\")\n","        elif candidate_matrix.join_key == \"session\":\n","            feat_names.extend(candidate_matrix.feat_name)\n","    df = df.filter(\n","        ~pl.all(pl.col(feat_names).is_null())\n","    )\n","    return df\n","\n","def negative_sample(df: pl.DataFrame) -\u003e pl.DataFrame:\n","    negatives = df.filter(df[\"label\"] == 0)\n","    negatives = negatives.sample(fraction=0.2, seed=SEED)\n","    df = pl.concat([df.filter(df[\"label\"] \u003e 0), negatives])\n","    return df\n","\n","def filter_session_not_include_positive(df: pl.DataFrame) -\u003e pl.DataFrame:\n","    positive_sessions = df.filter(pl.col(\"label\")==1)[\"session_id\"].to_list()\n","    df = df.filter(df[\"session_id\"].is_in(positive_sessions))\n","    return df\n","\n","def add_features(df: pl.DataFrame, session_feat_df:pl.DataFrame, product_feat_df:pl.DataFrame, i2v_models:Dict[str, Word2Vec], imf_model, user_id2index, item_id2index, graph_embs, item_id2indices_prone) -\u003e pl.DataFrame:\n","\n","    @njit()\n","    def calc_cos_sim(v1, v2):\n","        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","    # session features \n","    df = df.join(session_feat_df, on=\"session_id\", how=\"left\")\n","    \n","    # product features\n","    df = df.join(product_feat_df, left_on=[\"candidate_item\", \"locale\"], right_on=[\"id\", \"locale\"], how=\"left\")\n","\n","    # session * product features\n","    df = df.with_columns([\n","        (pl.col(\"P_price\") - pl.col(\"S_mean_price\")).alias(\"SP_price_diff_to_mean_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_min_price\")).alias(\"SP_price_diff_to_min_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_max_price\")).alias(\"SP_price_diff_to_max_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_price_diff_to_last_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_mean_price\")).alias(\"SP_brand_price_diff_to_mean_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_min_price\")).alias(\"SP_brand_price_diff_to_min_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_max_price\")).alias(\"SP_brand_price_diff_to_max_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_brand_price_diff_to_last_price\"),\n","    ])\n","\n","    # imf similarity between sessions and candidates\n","    sessions = df[\"session_id\"].to_list()\n","    candidates = df[\"candidate_item\"].to_list()\n","    imf_similarities = []\n","    user_index2vector = dict(enumerate(imf_model[\"user_factors\"]))\n","    item_index2vector = dict(enumerate(imf_model[\"item_factors\"]))\n","    for session, candidate in zip(sessions, candidates):\n","        try:\n","            user_index, item_index = user_id2index[session], item_id2index[candidate]\n","            v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n","            sim = calc_cos_sim(v1, v2)\n","        except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n","            sim = 0\n","        imf_similarities.append(np.float32(sim))\n","    df = df.with_columns(pl.Series(name=\"imf_similarity\", values=imf_similarities).cast(pl.Float32))\n","\n","    for last_n in LAST_NS:\n","        # item2vec similarity between last items and candidates\n","        dfs = []\n","        for locale in LOCALES:\n","            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","\n","            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n","            cand_items = df_by_locale[\"candidate_item\"].to_list()\n","            item_similalities = []\n","            for last_item, cand_item in zip(last_items, cand_items):\n","                try:\n","                    sim = i2v_models[locale].wv.similarity(last_item, cand_item)\n","                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n","                    sim = -1\n","                item_similalities.append(np.float32(sim))\n","            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"i2v_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n","            dfs.append(df_by_locale)\n","        df = pl.concat(dfs)\n","\n","        # prone similarity between last items and candidates\n","        dfs = []\n","        for locale in LOCALES:\n","            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n","            cand_items = df_by_locale[\"candidate_item\"].to_list()\n","            item_similalities = []\n","            item_index2vector = dict(enumerate(graph_embs[locale]))\n","            for last_item, cand_item in zip(last_items, cand_items):\n","                try:\n","                    item_index1 = item_id2indices_prone[locale][last_item]\n","                    item_index2 = item_id2indices_prone[locale][cand_item]\n","                    v1, v2 = item_index2vector[item_index1], item_index2vector[item_index2]\n","                    sim = calc_cos_sim(v1, v2)\n","                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n","                    sim = -1\n","                item_similalities.append(np.float32(sim))\n","            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"graph_emb_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n","            dfs.append(df_by_locale)\n","        df = pl.concat(dfs)\n","\n","    return df\n","\n","def fill_null_and_cast(df: pl.DataFrame) -\u003e pl.DataFrame:\n","    df = df.with_columns([\n","        pl.col(\"co_visit_weight_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"similarity_score_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"nns_similality_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last1\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"co_visit_weight_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"similarity_score_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"nns_similality_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last2\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"co_visit_weight_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"similarity_score_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"nns_similality_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last3\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"imf_score\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score_to_recent_intereaction_n3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score_to_recent_intereaction_n5\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score_to_recent_intereaction_n2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"co_visit_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"nns_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"co_visit_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"nns_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"co_visit_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"nns_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank_to_recent_intereaction_n3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank_to_recent_intereaction_n5\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank_to_recent_intereaction_n2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"S_locale\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"S_session_length\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_nunique_item\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_nunique_brand\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_color_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_size_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_model_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_material_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_author_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_ratio_unique_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_ratio_repurchase\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_std_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_last_item_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_total_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_n_unique_locale\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_color_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_size_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_model_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_material_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_author_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_brand_purchase_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"P_brand_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_total_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_brand_std_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_total_brand_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_locale_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_total_locale_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_amount_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_amount_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_price_diff_to_avg_brand_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_brand_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n","    ])\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"_FPA8z96Zil9"},"source":["## fix seed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wEckSTT1ZkDG"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","seed_everything(SEED)"]},{"cell_type":"markdown","metadata":{"id":"9d-OJvwkk8SN"},"source":["# Process data"]},{"cell_type":"markdown","metadata":{"id":"CgmlzAJyRXpD"},"source":["## train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4izMBylaa3Lq"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/7 [00:00\u003c?, ?it/s]\u003cipython-input-12-a69acb9b862e\u003e:96: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 1d, A))\n","  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","100%|██████████| 7/7 [1:52:28\u003c00:00, 964.03s/it]\n"]}],"source":["if MAKE_TRAIN:\n","    n_rows = 50_000\n","    for idx, df in tqdm(enumerate(train.iter_slices(n_rows=n_rows)), total=math.ceil(train.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar \n","        df = generate_candidates(df, candidate_matrices_train)\n","        df = df.drop(\"prev_items\")\n","        df = add_label(df)\n","        df = filter_null(df, candidate_matrices_train)\n","        df = filter_session_not_include_positive(df)\n","        df = negative_sample(df)\n","        df = add_features(df, session_feat, product_feat_train, i2v_models_train, imf_model_train, user_id2index_train, item_id2index_train, graph_embs_train, item_id2indices_prone_train)\n","        df = fill_null_and_cast(df)\n","        df.write_parquet(DIR + f\"data/interim/for_ranker/task2/train_chunk_{EXP_NAME}_{idx}\")"]},{"cell_type":"markdown","metadata":{"id":"3GmIYIArcVHW"},"source":["## test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5uffie7l3lQH"},"outputs":[],"source":["if MAKE_TEST:\n","    n_rows = 10_000\n","    for idx, df in tqdm(enumerate(test.iter_slices(n_rows=n_rows)), total=math.ceil(test.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar \n","        # process data\n","        df = generate_candidates(df, candidate_matrices_test)\n","        df = df.drop(\"prev_items\")\n","        df = add_features(df, session_feat, product_feat_test, i2v_models_test, imf_model_test, user_id2index_test, item_id2index_test, graph_embs_test, item_id2indices_prone_test)\n","        df = fill_null_and_cast(df)\n","        df.write_parquet(DIR + f\"data/interim/for_ranker/task2/test_chunk_{EXP_NAME}_{idx}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MUgmgpWJs-ir"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOdsFnh0aLAAqQfOtbRyXRk","machine_shape":"hm","name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}