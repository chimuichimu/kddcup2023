{"cells":[{"cell_type":"markdown","metadata":{"id":"v--J7LUqL9Cj"},"source":["# Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20708,"status":"ok","timestamp":1685976028408,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"BnKnD-UQWSEg","outputId":"0b033036-74a1-476d-884f-f44935f69f00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3951,"status":"ok","timestamp":1685976032349,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"8Af_47xxWVhC","outputId":"8cf28170-cef8-4b27-d96a-949a5d2a96f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (0.17.3)\n","Requirement already satisfied: typing_extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from polars) (4.5.0)\n"]}],"source":["!pip install polars"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34689,"status":"ok","timestamp":1685976067032,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"},"user_tz":-540},"id":"nWURCxBnSGcH","outputId":"8fca9253-0bba-4455-a2ce-b8cd1c39e0c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gensim==4.0.1\n","  Downloading gensim-4.0.1.tar.gz (23.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.22.4)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (1.10.1)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.0.1) (6.3.0)\n","Building wheels for collected packages: gensim\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for gensim\n","Failed to build gensim\n","\u001b[31mERROR: Could not build wheels for gensim, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install gensim==4.0.1"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"F-RBDJdwWdI-","executionInfo":{"status":"ok","timestamp":1685976071206,"user_tz":-540,"elapsed":4177,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"outputs":[],"source":["import os\n","import gc\n","import math\n","import random\n","from collections import defaultdict, Counter\n","from typing import List, Dict\n","import joblib\n","import pickle\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from numba import njit\n","import polars as pl\n","import pandas as pd\n","import lightgbm as lgb\n","from sklearn.model_selection import GroupKFold\n","from gensim.models import Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"hEPw4yzayxut"},"source":["## constants"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"c3OCFFMJZoNy","executionInfo":{"status":"ok","timestamp":1685976071206,"user_tz":-540,"elapsed":4,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"outputs":[],"source":["EXP_NAME = \"exp161\"\n","DIR = \"/gdrive/MyDrive/amazon_kdd_2023/\"\n","K_FOLDS = 3\n","SEED = 42\n","LOCALES = [\"IT\", \"FR\", \"ES\"]\n","MAKE_TRAIN = False\n","MAKE_VALID = False\n","MAKE_TEST = True\n","\n","# This parameter controls to which end item the candidate is tied. \n","# For example, if [1,2], candidates are generated from the last item and second last item in each session.\n","LAST_NS = [1, 2, 3] "]},{"cell_type":"code","source":["USE_FEATURES = [\n","    # === candidate features ===\n","    \"co_visit_weight_last1\", \"consective_1_weight_last1\", \"consective_3_weight_last1\", \"consective_5_weight_last1\", \"similarity_score_last1\", \"lift_last1\", \"prone_distance_last1\",\n","    \"co_visit_weight_last2\", \"consective_1_weight_last2\", \"consective_3_weight_last2\", \"consective_5_weight_last2\", \"similarity_score_last2\", \"lift_last2\", \"prone_distance_last2\",\n","    \"co_visit_weight_last3\", \"consective_1_weight_last3\", \"consective_3_weight_last3\", \"consective_5_weight_last3\", \"similarity_score_last3\", \"lift_last3\", \"prone_distance_last3\",\n","    \"imf_score\", \"imf_score_to_recent_intereaction_n3\", \"imf_score_to_recent_intereaction_n2\", \"imf_score_to_recent_intereaction_n5\",\n","    \"co_visit_rank_last1\", \"consective_1_rank_last1\", \"consective_3_rank_last1\", \"consective_5_rank_last1\", \"similarity_rank_last1\", \"lift_rank_last1\", \"prone_rank_last1\",\n","    \"co_visit_rank_last2\", \"consective_1_rank_last2\", \"consective_3_rank_last2\", \"consective_5_rank_last2\", \"similarity_rank_last2\", \"lift_rank_last2\", \"prone_rank_last2\",\n","    \"co_visit_rank_last3\", \"consective_1_rank_last3\", \"consective_3_rank_last3\", \"consective_5_rank_last3\", \"similarity_rank_last3\", \"lift_rank_last3\", \"prone_rank_last3\",\n","    \"imf_rank\", \"imf_rank_to_recent_intereaction_n3\", \"imf_rank_to_recent_intereaction_n2\", \"imf_rank_to_recent_intereaction_n5\",\n","    # === session features ===\n","    \"S_session_length\",\n","    \"S_nunique_brand\",\n","    \"S_ratio_unique_brand\",\n","    \"S_nunique_item\",\n","    \"S_ratio_repurchase\",\n","    \"S_locale\",\n","    \"S_mean_price\", \"S_max_price\", \"S_min_price\", \"S_std_price\", \"S_total_amount\",\n","    \"S_color_not_null_count\", \"S_size_not_null_count\", \"S_model_not_null_count\", \"S_material_not_null_count\", \"S_author_not_null_count\",\n","    \"S_last_item_price\",\n","    # === product features ===\n","    \"P_price\", \"P_locale_purchase_count\", \"P_total_locale_amount\",\n","    \"P_purchase_count\", \"P_purchase_count_global\",\n","    \"P_total_amount\",\n","    \"P_brand_purchase_count\", \"P_brand_purchase_count_global\",\n","    \"P_brand_mean_price\", \"P_brand_max_price\", \"P_brand_min_price\", \"P_brand_std_price\", \"P_total_brand_amount\",\n","    \"P_price_diff_to_avg_brand_price\",\n","    \"P_n_unique_locale\",\n","    \"P_is_color_null\", \"P_is_size_null\", \"P_is_model_null\", \"P_is_material_null\", \"P_is_author_null\",\n","    \"P_purchase_count_ratio_to_locale\", \"P_purchase_amount_ratio_to_locale\", \"P_purchase_count_ratio_to_brand\", \"P_purchase_amount_ratio_to_brand\",\n","    # === session * product features ===\n","    \"SP_price_diff_to_mean_price\", \"SP_price_diff_to_min_price\", \"SP_price_diff_to_max_price\", \"SP_price_diff_to_last_price\",\n","    \"SP_brand_price_diff_to_mean_price\", \"SP_brand_price_diff_to_min_price\", \"SP_brand_price_diff_to_max_price\", \"SP_brand_price_diff_to_last_price\",\n","    \"SP_same_brand_last1\", \"SP_same_brand_last2\", \"SP_same_brand_last3\",\n","    \"SP_same_color_last1\", \"SP_same_color_last2\", \"SP_same_color_last3\",\n","    \"SP_same_size_last1\", \"SP_same_size_last2\", \"SP_same_size_last3\",\n","    \"SP_same_model_last1\", \"SP_same_model_last2\", \"SP_same_model_last3\",\n","    \"SP_same_material_last1\", \"SP_same_material_last2\", \"SP_same_material_last3\",\n","    \"SP_same_author_last1\", \"SP_same_author_last2\", \"SP_same_author_last3\",\n","    \"SP_same_brand_sum\", \"SP_same_color_sum\", \"SP_same_size_sum\", \"SP_same_model_sum\", \"SP_same_material_sum\", \"SP_same_author_sum\",\n","    # === similality features ===\n","    \"imf_similarity\", \n","    \"graph_emb_similarity_last1\", \"graph_emb_similarity_last2\", \"graph_emb_similarity_last3\", \n","    \"i2v_similarity_last1\", \"i2v_similarity_last2\", \"i2v_similarity_last3\", \n","]"],"metadata":{"id":"i_EZKHvQN6T-","executionInfo":{"status":"ok","timestamp":1685976071206,"user_tz":-540,"elapsed":4,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f19D1NtjyBUk"},"source":["## load data"]},{"cell_type":"code","source":["class CandidateMatrix:\n","    def __init__(self, matrix: pl.DataFrame, feat_name: List[str], join_key: str):\n","        self.matrix = matrix\n","        self.feat_name = feat_name\n","        self.join_key = join_key"],"metadata":{"id":"Gy094WXkDJVb","executionInfo":{"status":"ok","timestamp":1685976071207,"user_tz":-540,"elapsed":4,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"LOiuD_bFC4jA","executionInfo":{"status":"ok","timestamp":1685976073424,"user_tz":-540,"elapsed":2221,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"outputs":[],"source":["# session data\n","train = pl.read_parquet(DIR + \"data/preprocessed/task2/train_task2.parquet\")\n","test = pl.read_parquet(DIR + \"data/preprocessed/task2/test_task2_phase2.parquet\")"]},{"cell_type":"code","source":["# feature data\n","session_feat = pl.read_parquet(DIR + \"data/interim/features/task2/session_feature_09.parquet\")\n","product_feat_train = pl.read_parquet(DIR + \"data/interim/features/task2/product_feature_train_12.parquet\")\n","product_feat_test = pl.read_parquet(DIR + \"data/interim/features/task2/product_feature_test_12.parquet\")"],"metadata":{"id":"drzF-AZlDLso","executionInfo":{"status":"ok","timestamp":1685976078335,"user_tz":-540,"elapsed":4914,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["similar_products = pl.read_parquet(DIR + \"data/interim/candidates/task2/similar_products_17.parquet\")"],"metadata":{"id":"illkwaxrDQDV","executionInfo":{"status":"ok","timestamp":1685976087202,"user_tz":-540,"elapsed":8880,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["if MAKE_TRAIN or MAKE_VALID:\n","    imf_candidates_train_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_31_for_train_or_eval.parquet\")\n","    imf_candidates_train_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_32_for_train_or_eval.parquet\")\n","    imf_candidates_train_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_33_for_train_or_eval.parquet\")\n","    imf_candidates_train_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_34_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_32_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_33_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_34_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_35_for_train_or_eval.parquet\")\n","    co_visit_matrix_train_5 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_36_for_train_or_eval.parquet\")\n","    prone_matrix_train = pl.read_parquet(DIR + \"data/interim/candidates/task2/prone_07_for_local_or_eval.parquet\")\n","\n","    candidate_matrices_train =[\n","        CandidateMatrix(co_visit_matrix_train_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_2, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_3, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_4, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_train_5, [\"lift\", \"lift_rank\"], \"item\"),\n","        CandidateMatrix(similar_products, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n","        CandidateMatrix(imf_candidates_train_1, [\"imf_score\", \"imf_rank\"], \"session\"),\n","        CandidateMatrix(imf_candidates_train_2, [\"imf_score_to_recent_intereaction_n3\", \"imf_rank_to_recent_intereaction_n3\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_train_3, [\"imf_score_to_recent_intereaction_n5\", \"imf_rank_to_recent_intereaction_n5\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_train_4, [\"imf_score_to_recent_intereaction_n2\", \"imf_rank_to_recent_intereaction_n2\"], \"session\"),    \n","        CandidateMatrix(prone_matrix_train, [\"prone_distance\", \"prone_rank\"], \"item\"),    \n","    ]\n","\n","    # item2vec model\n","    i2v_models_train = {}\n","    for locale in LOCALES:\n","        i2v_models_train[locale] = Word2Vec.load(DIR + f\"models/task2/item2vec_{locale}_16_for_train_or_eval.model\")\n","\n","    # imf model\n","    imf_model_train = np.load(DIR + \"models/task2/imf_31_model_for_train_or_eval.npz\")\n","    with open(DIR + \"models/task2/imf_31_user_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","        user_id2index_train = pickle.load(f)\n","    with open(DIR + \"models/task2/imf_31_item_id2index_for_train_or_eval.pickle\", \"rb\") as f:\n","        item_id2index_train = pickle.load(f)\n","    \n","    # prone\n","    graph_embs_train = {}\n","    item_id2indices_prone_train = {}\n","    for locale in LOCALES:\n","        graph_embs_train[locale] = np.load(DIR + f\"models/task2/graph_embedding_07_{locale}_for_local_train_or_eval.npy\")\n","        with open(DIR + \"data/interim/graph/task2/graph_\" + f\"item_id2index_07_{locale}_for_train_or_eval.pickle\", \"rb\") as f:\n","            item_id2indices_prone_train[locale] = pickle.load(f)"],"metadata":{"id":"8zYqjY06DSYr","executionInfo":{"status":"ok","timestamp":1685976087203,"user_tz":-540,"elapsed":8,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["if MAKE_TEST:\n","    imf_candidates_test_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_31_for_inference.parquet\")\n","    imf_candidates_test_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_32_for_inference.parquet\")\n","    imf_candidates_test_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_33_for_inference.parquet\")\n","    imf_candidates_test_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/imf_34_for_inference.parquet\")\n","    co_visit_matrix_test_1 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_32_for_inference.parquet\")\n","    co_visit_matrix_test_2 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_33_for_inference.parquet\")\n","    co_visit_matrix_test_3 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_34_for_inference.parquet\")\n","    co_visit_matrix_test_4 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_35_for_inference.parquet\")\n","    co_visit_matrix_test_5 = pl.read_parquet(DIR + \"data/interim/candidates/task2/co_visit_matrix_36_for_inference.parquet\")\n","    prone_matrix_test = pl.read_parquet(DIR + \"data/interim/candidates/task2/prone_07_for_inference.parquet\")\n","\n","    candidate_matrices_test =[\n","        CandidateMatrix(co_visit_matrix_test_1, [\"co_visit_weight\", \"co_visit_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_2, [\"consective_1_weight\", \"consective_1_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_3, [\"consective_3_weight\", \"consective_3_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_4, [\"consective_5_weight\", \"consective_5_rank\"], \"item\"),\n","        CandidateMatrix(co_visit_matrix_test_5, [\"lift\", \"lift_rank\"], \"item\"),\n","        CandidateMatrix(similar_products, [\"similarity_score\", \"similarity_rank\"], \"item\"),\n","        CandidateMatrix(imf_candidates_test_1, [\"imf_score\", \"imf_rank\"], \"session\"),\n","        CandidateMatrix(imf_candidates_test_2, [\"imf_score_to_recent_intereaction_n3\", \"imf_rank_to_recent_intereaction_n3\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_test_3, [\"imf_score_to_recent_intereaction_n5\", \"imf_rank_to_recent_intereaction_n5\"], \"session\"),    \n","        CandidateMatrix(imf_candidates_test_4, [\"imf_score_to_recent_intereaction_n2\", \"imf_rank_to_recent_intereaction_n2\"], \"session\"),    \n","        CandidateMatrix(prone_matrix_test, [\"prone_distance\", \"prone_rank\"], \"item\"),    \n","    ]\n","    \n","    # item2vec \n","    i2v_models_test = {}\n","    for locale in LOCALES:\n","        i2v_models_test[locale] = Word2Vec.load(DIR + f\"models/task2/item2vec_{locale}_16_for_inference.model\")\n","\n","    # imf model\n","    imf_model_test = np.load(DIR + \"models/task2/imf_31_model_for_inference.npz\")\n","    with open(DIR + \"models/task2/imf_31_user_id2index_for_inference.pickle\", \"rb\") as f:\n","        user_id2index_test = pickle.load(f)\n","    with open(DIR + \"models/task2/imf_31_item_id2index_for_inference.pickle\", \"rb\") as f:\n","        item_id2index_test = pickle.load(f)\n","\n","    # prone\n","    graph_embs_test = {}\n","    item_id2indices_prone_test = {}\n","    for locale in LOCALES:\n","        graph_embs_test[locale] = np.load(DIR + f\"models/task2/graph_embedding_07_{locale}_for_inference.npy\")\n","        with open(DIR + \"data/interim/graph/task2/graph_\" + f\"item_id2index_07_{locale}_for_inference.pickle\", \"rb\") as f:\n","            item_id2indices_prone_test[locale] = pickle.load(f)"],"metadata":{"id":"5Sak9XgADSbP","executionInfo":{"status":"ok","timestamp":1685976135989,"user_tz":-540,"elapsed":48792,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-NbXCWfyDZt"},"source":["## functions"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"D75acjHUyEyt","executionInfo":{"status":"ok","timestamp":1685976135991,"user_tz":-540,"elapsed":10,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"outputs":[],"source":["# functions for data processing\n","\n","def generate_candidates(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n","\n","    def add_last_n_item(df: pl.DataFrame, last_n: int) -> pl.DataFrame:\n","        last_item_list = []\n","        prev_items_list = df[\"prev_items\"].to_list()\n","        for prev_items in prev_items_list:\n","            try:\n","                last_item_list.append(prev_items[-last_n])\n","            except IndexError:\n","                last_item_list.append(None)\n","        df = df.with_columns(pl.Series(name=f\"last_item_{last_n}\", values=last_item_list))\n","        return df\n","\n","    # add last_item columns\n","    for last_n in LAST_NS:\n","        df = add_last_n_item(df, last_n)\n","\n","    # generate candidates\n","    candidates = []\n","\n","    # candidates tied to items\n","    for last_n in LAST_NS:\n","        for candidate_matrix in candidate_matrices:\n","            if candidate_matrix.join_key == \"item\":\n","                # join candidates to last_n item\n","                candidate = df.join(candidate_matrix.matrix, left_on=[f\"last_item_{last_n}\", \"locale\"], right_on=[\"item\", \"locale\"], how=\"left\")\n","                candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n","\n","                # keep candidates for feature addition later\n","                original_feat_names = candidate_matrix.feat_name\n","                feat_names = [f\"{x}_last{last_n}\" for x in original_feat_names]\n","                tmp = candidate[[\"session_id\", \"candidate_item\"] + original_feat_names]\n","                for original_feat_name, feat_name in zip(original_feat_names, feat_names):\n","                    tmp = tmp.rename({original_feat_name:feat_name})\n","                candidates.append(tmp)\n","\n","    # candidates tied to session\n","    for candidate_matrix in candidate_matrices:\n","        if candidate_matrix.join_key == \"session\":\n","            # join candidates to session\n","            candidate = df.join(candidate_matrix.matrix, on=\"session_id\", how=\"left\")\n","            candidate = candidate.filter(~pl.col(\"candidate_item\").is_in(pl.col(\"prev_items\"))) # remove already purchased items\n","\n","            # keep candidates for feature addition later\n","            candidates.append(candidate[[\"session_id\", \"candidate_item\"] + candidate_matrix.feat_name])\n","\n","    cand_all = pl.concat([df[[\"session_id\", \"candidate_item\"]] for df in candidates])\n","\n","    # remove duplicate candidates\n","    cand_all = cand_all.unique(subset=[\"session_id\", \"candidate_item\"])\n","\n","    # join candidates\n","    df = df.join(cand_all, on=[\"session_id\"], how=\"left\")\n","\n","    # add features derived from the candidate\n","    for candidate in candidates:\n","        df = df.join(candidate, on=[\"session_id\", \"candidate_item\"], how=\"left\")\n","\n","    return df\n","\n","def add_label(df: pl.DataFrame) -> pl.DataFrame:\n","    df = df.with_columns((pl.col(\"candidate_item\") == pl.col(\"next_item\")).cast(pl.Int8).alias(\"label\"))\n","    return df\n","\n","def filter_null(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n","    feat_names = []\n","    for candidate_matrix in candidate_matrices:\n","        if candidate_matrix.join_key == \"item\":\n","            for last_n in LAST_NS:\n","                for feat_name in candidate_matrix.feat_name:\n","                    feat_names.append(f\"{feat_name}_last{last_n}\")\n","        elif candidate_matrix.join_key == \"session\":\n","            feat_names.extend(candidate_matrix.feat_name)\n","    df = df.filter(\n","        ~pl.all(pl.col(feat_names).is_null())\n","    )\n","    return df\n","\n","def negative_sample(df: pl.DataFrame) -> pl.DataFrame:\n","    negatives = df.filter(df[\"label\"] == 0)\n","    negatives = negatives.sample(fraction=0.2, seed=SEED)\n","    df = pl.concat([df.filter(df[\"label\"] > 0), negatives])\n","    return df\n","\n","def filter_session_not_include_positive(df: pl.DataFrame) -> pl.DataFrame:\n","    positive_sessions = df.filter(pl.col(\"label\")==1)[\"session_id\"].to_list()\n","    df = df.filter(df[\"session_id\"].is_in(positive_sessions))\n","    return df\n","\n","def add_features(df: pl.DataFrame, session_feat_df:pl.DataFrame, product_feat_df:pl.DataFrame, i2v_models:Dict[str, Word2Vec], imf_model, user_id2index, item_id2index, graph_embs, item_id2indices_prone) -> pl.DataFrame:\n","\n","    @njit()\n","    def calc_cos_sim(v1, v2):\n","        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","    # session features \n","    df = df.join(session_feat_df, on=\"session_id\", how=\"left\")\n","    \n","    # product features\n","    df = df.join(product_feat_df, left_on=[\"candidate_item\", \"locale\"], right_on=[\"id\", \"locale\"], how=\"left\")\n","\n","    # session * product features\n","    df = df.with_columns([\n","        (pl.col(\"P_price\") - pl.col(\"S_mean_price\")).alias(\"SP_price_diff_to_mean_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_min_price\")).alias(\"SP_price_diff_to_min_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_max_price\")).alias(\"SP_price_diff_to_max_price\"),\n","        (pl.col(\"P_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_price_diff_to_last_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_mean_price\")).alias(\"SP_brand_price_diff_to_mean_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_min_price\")).alias(\"SP_brand_price_diff_to_min_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_max_price\")).alias(\"SP_brand_price_diff_to_max_price\"),\n","        (pl.col(\"P_brand_mean_price\") - pl.col(\"S_last_item_price\")).alias(\"SP_brand_price_diff_to_last_price\"),\n","    ])\n","\n","    for last_n in LAST_NS:\n","        df = df.with_columns([\n","            ((pl.col(\"P_brand\") == pl.col(f\"S_brand_last{last_n}\"))&(pl.col(f\"S_brand_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_brand_last{last_n}\"),\n","            ((pl.col(\"P_color\") == pl.col(f\"S_color_last{last_n}\"))&(pl.col(f\"S_color_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_color_last{last_n}\"),\n","            ((pl.col(\"P_size\") == pl.col(f\"S_size_last{last_n}\"))&(pl.col(f\"S_size_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_size_last{last_n}\"),\n","            ((pl.col(\"P_model\") == pl.col(f\"S_model_last{last_n}\"))&(pl.col(f\"S_model_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_model_last{last_n}\"),\n","            ((pl.col(\"P_material\") == pl.col(f\"S_material_last{last_n}\"))&(pl.col(f\"S_material_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_material_last{last_n}\"),\n","            ((pl.col(\"P_author\") == pl.col(f\"S_author_last{last_n}\"))&(pl.col(f\"S_author_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_author_last{last_n}\"),\n","        ])\n","    df = df.with_columns([\n","        (pl.col(\"SP_same_brand_last1\") + pl.col(\"SP_same_brand_last2\") + pl.col(\"SP_same_brand_last3\")).cast(pl.UInt8).alias(\"SP_same_brand_sum\"),\n","        (pl.col(\"SP_same_color_last1\") + pl.col(\"SP_same_color_last2\") + pl.col(\"SP_same_color_last3\")).cast(pl.UInt8).alias(\"SP_same_color_sum\"),\n","        (pl.col(\"SP_same_size_last1\") + pl.col(\"SP_same_size_last2\") + pl.col(\"SP_same_size_last3\")).cast(pl.UInt8).alias(\"SP_same_size_sum\"),\n","        (pl.col(\"SP_same_model_last1\") + pl.col(\"SP_same_model_last2\") + pl.col(\"SP_same_model_last3\")).cast(pl.UInt8).alias(\"SP_same_model_sum\"),\n","        (pl.col(\"SP_same_material_last1\") + pl.col(\"SP_same_material_last2\") + pl.col(\"SP_same_material_last3\")).cast(pl.UInt8).alias(\"SP_same_material_sum\"),\n","        (pl.col(\"SP_same_author_last1\") + pl.col(\"SP_same_author_last2\") + pl.col(\"SP_same_author_last3\")).cast(pl.UInt8).alias(\"SP_same_author_sum\"),\n","    ])\n","\n","    # imf similarity between sessions and candidates\n","    sessions = df[\"session_id\"].to_list()\n","    candidates = df[\"candidate_item\"].to_list()\n","    imf_similarities = []\n","    user_index2vector = dict(enumerate(imf_model[\"user_factors\"]))\n","    item_index2vector = dict(enumerate(imf_model[\"item_factors\"]))\n","    for session, candidate in zip(sessions, candidates):\n","        try:\n","            user_index, item_index = user_id2index[session], item_id2index[candidate]\n","            v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n","            sim = calc_cos_sim(v1, v2)\n","        except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n","            sim = 0\n","        imf_similarities.append(np.float32(sim))\n","    df = df.with_columns(pl.Series(name=\"imf_similarity\", values=imf_similarities).cast(pl.Float32))\n","\n","    for last_n in LAST_NS:\n","        # item2vec similarity between last items and candidates\n","        dfs = []\n","        for locale in LOCALES:\n","            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","\n","            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n","            cand_items = df_by_locale[\"candidate_item\"].to_list()\n","            item_similalities = []\n","            for last_item, cand_item in zip(last_items, cand_items):\n","                try:\n","                    sim = i2v_models[locale].wv.similarity(last_item, cand_item)\n","                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n","                    sim = -1\n","                item_similalities.append(np.float32(sim))\n","            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"i2v_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n","            dfs.append(df_by_locale)\n","        df = pl.concat(dfs)\n","\n","        # prone similarity between last items and candidates\n","        dfs = []\n","        for locale in LOCALES:\n","            df_by_locale = df.filter(pl.col(\"locale\") == locale)\n","            last_items = df_by_locale[f\"last_item_{last_n}\"].to_list()\n","            cand_items = df_by_locale[\"candidate_item\"].to_list()\n","            item_similalities = []\n","            item_index2vector = dict(enumerate(graph_embs[locale]))\n","            for last_item, cand_item in zip(last_items, cand_items):\n","                try:\n","                    item_index1 = item_id2indices_prone[locale][last_item]\n","                    item_index2 = item_id2indices_prone[locale][cand_item]\n","                    v1, v2 = item_index2vector[item_index1], item_index2vector[item_index2]\n","                    sim = calc_cos_sim(v1, v2)\n","                except (KeyError, TypeError): # KeyError if the item is not in the item2vec training data. TypeError if there are no candidates in a session.\n","                    sim = -1\n","                item_similalities.append(np.float32(sim))\n","            df_by_locale = df_by_locale.with_columns(pl.Series(name=f\"graph_emb_similarity_last{last_n}\", values=item_similalities).cast(pl.Float32))\n","            dfs.append(df_by_locale)\n","        df = pl.concat(dfs)\n","\n","    return df\n","\n","def fill_null_and_cast(df: pl.DataFrame) -> pl.DataFrame:\n","    df = df.with_columns([\n","        pl.col(\"co_visit_weight_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last1\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"similarity_score_last1\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last1\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"co_visit_weight_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last2\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"similarity_score_last2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last2\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"co_visit_weight_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"consective_1_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_3_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"consective_5_weight_last3\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"lift_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"similarity_score_last3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"prone_distance_last3\").fill_null(-1).cast(pl.Float32),\n","        pl.col(\"imf_score\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score_to_recent_intereaction_n3\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score_to_recent_intereaction_n5\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"imf_score_to_recent_intereaction_n2\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"co_visit_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last1\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"co_visit_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"co_visit_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_1_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_3_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"consective_5_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"lift_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"similarity_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"prone_rank_last3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank_to_recent_intereaction_n3\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank_to_recent_intereaction_n5\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"imf_rank_to_recent_intereaction_n2\").fill_null(999).cast(pl.UInt16),\n","        pl.col(\"S_locale\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"S_session_length\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_nunique_item\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_nunique_brand\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_color_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_size_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_model_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_material_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_author_not_null_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"S_ratio_unique_brand\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"S_ratio_repurchase\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"S_mean_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"S_max_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"S_min_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"S_std_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"S_last_item_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"S_total_amount\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n","        pl.col(\"P_n_unique_locale\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_color_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_size_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_model_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_material_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_is_author_null\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"P_brand_purchase_count\").fill_null(0).cast(pl.UInt16),\n","        pl.col(\"P_brand_purchase_count_global\").fill_null(0).cast(pl.UInt32),\n","        # pl.col(\"P_total_amount\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_brand_mean_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_brand_max_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_brand_min_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_brand_std_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_total_brand_amount\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"P_locale_purchase_count\").fill_null(0).cast(pl.UInt32),\n","        # pl.col(\"P_total_locale_amount\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_purchase_count_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_purchase_amount_ratio_to_locale\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_purchase_count_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_purchase_amount_ratio_to_brand\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"P_price_diff_to_avg_brand_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_brand_price_diff_to_mean_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_brand_price_diff_to_min_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_brand_price_diff_to_max_price\").fill_null(0).cast(pl.Float32),\n","        # pl.col(\"SP_brand_price_diff_to_last_price\").fill_null(0).cast(pl.Float32),\n","        pl.col(\"SP_same_brand_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_brand_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_brand_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_last1\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_last2\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_last3\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_brand_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_color_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_size_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_model_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_material_sum\").fill_null(0).cast(pl.UInt8),\n","        pl.col(\"SP_same_author_sum\").fill_null(0).cast(pl.UInt8),\n","    ])\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"_FPA8z96Zil9"},"source":["## fix seed"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"wEckSTT1ZkDG","executionInfo":{"status":"ok","timestamp":1685976135992,"user_tz":-540,"elapsed":9,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","seed_everything(SEED)"]},{"cell_type":"markdown","source":["# Process data"],"metadata":{"id":"9d-OJvwkk8SN"}},{"cell_type":"markdown","metadata":{"id":"CgmlzAJyRXpD"},"source":["## train"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"4izMBylaa3Lq","executionInfo":{"status":"ok","timestamp":1685976135992,"user_tz":-540,"elapsed":9,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"outputs":[],"source":["if MAKE_TRAIN:\n","    n_rows = 50_000\n","    for idx, df in tqdm(enumerate(train.iter_slices(n_rows=n_rows)), total=math.ceil(train.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar \n","        df = generate_candidates(df, candidate_matrices_train)\n","        df = df.drop(\"prev_items\")\n","        df = add_label(df)\n","        df = filter_null(df, candidate_matrices_train)\n","        df = filter_session_not_include_positive(df)\n","        df = negative_sample(df)\n","        df = add_features(df, session_feat, product_feat_train, i2v_models_train, imf_model_train, user_id2index_train, item_id2index_train, graph_embs_train, item_id2indices_prone_train)\n","        df = fill_null_and_cast(df)\n","        df = df[[\"session_id\", \"candidate_item\", \"label\"] + USE_FEATURES]\n","        df.write_parquet(DIR + f\"data/interim/for_ranker/task2/train_chunk_{EXP_NAME}_{idx}\")"]},{"cell_type":"markdown","metadata":{"id":"3GmIYIArcVHW"},"source":["## test"]},{"cell_type":"code","source":["if MAKE_TEST:\n","    n_rows = 10_000\n","    for idx, df in tqdm(enumerate(test.iter_slices(n_rows=n_rows)), total=math.ceil(test.height/n_rows)): # specify \"total\" parameter to display tqdm progress bar \n","        # process data\n","        df = generate_candidates(df, candidate_matrices_test)\n","        df = df.drop(\"prev_items\")\n","        df = add_features(df, session_feat, product_feat_test, i2v_models_test, imf_model_test, user_id2index_test, item_id2index_test, graph_embs_test, item_id2indices_prone_test)\n","        df = fill_null_and_cast(df)\n","        df.write_parquet(DIR + f\"data/interim/for_ranker/task2/test_chunk_{EXP_NAME}_{idx}\")"],"metadata":{"id":"5uffie7l3lQH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685979396195,"user_tz":-540,"elapsed":3260212,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}},"outputId":"0b4fe560-d43b-4624-9bc7-ca1e7381be5d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/4 [00:00<?, ?it/s]<ipython-input-13-23e2999e636c>:96: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 1d, A))\n","  return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","100%|██████████| 4/4 [54:20<00:00, 815.05s/it]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MUgmgpWJs-ir","executionInfo":{"status":"ok","timestamp":1685979396196,"user_tz":-540,"elapsed":12,"user":{"displayName":"chimuichimu","userId":"08341172728947671838"}}},"execution_count":16,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNU+VW/hCMmAhtItkerwiD7"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}